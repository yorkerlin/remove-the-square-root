# remove-the-square-root

PyTorch implementation of our square-root-free adaptive methods based on [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective (ICML 2024)](https://arxiv.org/abs/2402.03496)

Todo
* add the root-free RMSProp and IF-Shampoo
* add NN models and training scripts considered in our paper
* add the HyperParameter (HP) search space for each adaptive method (in the second stage) and the optimal HPs used in our paper
